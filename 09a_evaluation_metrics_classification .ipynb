{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Type | Supervision | Output | Main Aim | Example | Algorithm |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| `Regression` | Supervised | Numeric | Forecast / Predict | Predicting stock market price | Linear Regression |\n",
    "| `Classification` | Supervised | Categorical | Compute the category of the data | Classify emails as spam or non-spam | Logistic Regression |\n",
    "| `Clustering` | Unsupervised | Assigns data points into clusters | Group similar items into clusters | Find all transactions which are fraudulent in nature | K-Means |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Classification:**\n",
    "1. Binary Classification\n",
    "2. Multi-Class Classification\n",
    "3. Multi-Label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Evaluation Metrics for `Classification` Models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Confusion Matrix:**\n",
    "[Click to read the Blog for more clear understanding](https://medium.com/analytics-vidhya/machine-learning-metrics-in-simple-terms-d58a9c85f9f6)\n",
    "> **Descrption**: A confusion matrix is a table used in machine learning and statistics to assess the performance of a classification model. It summarizes the results of classification by showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "\n",
    "- **True Positive (TP)**: The number of correct positive predictions.\n",
    "- **True Negative (TN)**: The number of correct negative predictions.\n",
    "- **False Positive (FP)**: The number of incorrect positive predictions.\n",
    "- **False Negative (FN)**: The number of incorrect negative predictions.\n",
    "\n",
    "### **2.2 Accuracy:**\n",
    "> **Descrption**: Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model.\n",
    "\n",
    "- **Accuracy = (TP + TN) / (TP + TN + FP + FN)**\n",
    "- **Range**: 0 to 1\n",
    "- **Higher the value, better the model.**\n",
    "- **Drawback**: It works well only if there are equal number of samples belonging to each class.\n",
    "- **Example**: If there are 100 samples, and 95 are correctly classified, then the accuracy is 95%.\n",
    "- **Python Code**: `accuracy_score(y_true, y_pred)`\n",
    "\n",
    "### **2.3 Precision:**\n",
    "> **Descrption**: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate.\n",
    "\n",
    "- **Precision = TP / (TP + FP)**\n",
    "- **Range**: 0 to 1 (1 is the best)\n",
    "- **Higher the value, better the model.\n",
    "- **Drawback**: Precision should be used when the cost of False Positive is high.\n",
    "- **Example**: If there are 100 samples, and 95 are correctly classified, then the precision is 95%.\n",
    "- **Python Code**: `precision_score(y_true, y_pred)`\n",
    "\n",
    "### **2.4 Recall (Sensitivity):**\n",
    "> **Descrption**: Recall is the ratio of correctly predicted positive observations to the all observations in actual class. The question recall answers is: Of all the passengers that truly survived, how many did we label? It is also called Sensitivity.\n",
    "\n",
    "- **Recall = TP / (TP + FN)**\n",
    "- **Range**: 0 to 1 (1 is the best)\n",
    "- **Higher the value, better the model.\n",
    "- **Drawback**: Recall should be used when the cost of False Negative is high.\n",
    "- **Example**: If there are 100 samples, and 95 are correctly classified, then the recall is 95%.\n",
    "- **Python Code**: `recall_score(y_true, y_pred)`\n",
    "- **Note**: Recall is the most important metric for the model.\n",
    "\n",
    "### **2.5 F1 Score:**\n",
    "> **Descrption**: F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is a good way to show that a classifer has a good value for both recall and precision.\n",
    "\n",
    "- **F1 Score = 2 * (Precision * Recall) / (Precision + Recall)**\n",
    "- **Range**: 0 to 1 (1 is the best)\n",
    "- **Higher the value, better the model.\n",
    "- **Drawback**: F1 Score should be used when the cost of False Positive and False Negative are high.\n",
    "- **Example**: If there are 100 samples, and 95 are correctly classified, then the F1 Score is 95%.\n",
    "- **Python Code**: `f1_score(y_true, y_pred)`\n",
    "\n",
    "### **2.6 Specificity:**\n",
    "> **Descrption**: Specificity is the ratio of correctly predicted negative observations to the all observations in actual class. It is also called True Negative Rate.\n",
    "\n",
    "- **Specificity = TN / (TN + FP)**\n",
    "- **Range**: 0 to 1 (1 is the best)\n",
    "- **Higher the value, better the model.\n",
    "- **Drawback**: Specificity should be used when the cost of False Positive is high.\n",
    "- **Example**: If there are 100 samples, and 95 are correctly classified, then the specificity is 95%.\n",
    "- **Python Code**: `specificity_score(y_true, y_pred)`\n",
    "\n",
    "### **2.7 ROC Curve (Receiver Operating Characteristic):**\n",
    "> **Descrption**: The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection in machine learning. The false-positive rate is also known as probability of false alarm and can be calculated as (1 âˆ’ specificity).\n",
    "\n",
    "- **Range**: 0 to 1 (1 is the best)\n",
    "- **Higher the value, better the model.\n",
    "- **Drawback**: ROC Curve should be used when the cost of False Positive and False Negative are high.\n",
    "- **Python Code**: `roc_curve(y_true, y_pred)`\n",
    "- **Note**: The area under the ROC curve (AUC) is a single scalar value that summarizes the performance of the classifier across all threshold values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Choosing the Right Metric for Classification Models:**\n",
    "- **Accuracy**: When the cost of False Positive and False Negative are almost same.\n",
    "- **Precision**: When the cost of False Positive is high.\n",
    "- **Recall**: When the cost of False Negative is high.\n",
    "- **F1 Score**: When the cost of False Positive and False Negative are high.\n",
    "- **Specificity**: When the cost of False Positive is high.\n",
    "- **ROC Curve**: When the cost of False Positive and False Negative are high.\n",
    "- **AUC**: When the cost of False Positive and False Negative are high.\n",
    "\n",
    "**Note**: The choice of metric depends on the business problem and the cost of False Positive and False Negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of Evaluation Metrics for Classification Models:**\n",
    "| Metric | Pros | Cons | Example |\n",
    "| --- | --- | --- | --- |\n",
    "| Accuracy | Easy to understand. Directly tells you of the total predictions, how many were correct. | Can be misleading if classes are imbalanced. | If Accuracy is 1, the model is perfect. If Accuracy is 0.5, the model is correct 50% of the time. |\n",
    "| Precision | Measures the quality of the prediction when it predicts the positive class. | Does not consider true negatives. Can be misleading if classes are imbalanced. | If Precision is 1, when the model predicts positive, it is always correct. If Precision is 0.5, when the model predicts positive, it is correct 50% of the time. |\n",
    "| Recall (Sensitivity) | Measures the ability of the model to find all the positive samples. | Does not consider true negatives. Can be misleading if classes are imbalanced. | If Recall is 1, the model identifies all actual positives correctly. If Recall is 0.5, the model identifies 50% of actual positives correctly. |\n",
    "| F1-Score | Harmonic mean of Precision and Recall. An F1 Score reaches its best value at 1 (perfect precision and recall) and worst at 0. | F1 Score might not be a good measure to use if you care very much about precision or recall over the other. | If F1-Score is 1, the model has perfect precision and recall. If F1-Score is 0.5, the model has 50% precision and recall. |\n",
    "| Area under the ROC Curve (AUC-ROC) | AUC-ROC measures the entire two-dimensional area underneath the entire ROC curve. AUC-ROC score of 1 represents a perfect model; an area of 0.5 represents a model that is no better than random. | AUC-ROC can be overly optimistic if classes are highly imbalanced. | If AUC-ROC is 1, the model is perfect. If AUC-ROC is 0.5, the model is no better than random. |\n",
    "| Confusion Matrix | Gives a detailed breakdown of prediction errors. Shows the types of errors made by the model. | Can be hard to interpret for more than two classes. | A confusion matrix is a table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class. |\n",
    "\n",
    "`Remember, the best metric to use depends on the specific characteristics of your classification problem, such as the presence of imbalanced classes.`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
