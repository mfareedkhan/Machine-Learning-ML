{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensembled Algorithms**\n",
    "> Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. \n",
    "The main principle behind ensemble methods is that a group of weak learners come together to form a strong learner. \n",
    "\n",
    "The main types of ensemble methods are:\n",
    "1. Bagging (Bootstrap Aggregating)\n",
    "2. Boosting (AdaBoost, Gradient Boosting)\n",
    "3. Stacking (Stacked Generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Bagging**:\n",
    "> Bagging is a method in ensemble for improving unstable estimation or classification schemes. Bagging both can reduce errors by reducing the variance term.\n",
    "\n",
    "**A Layman Example:**\n",
    "\n",
    "Suppose you want to `buy a car` and you ask for opinions from several friends. Then, you make a list of the cars that are recommended most. Finally, you select the car that is most recommended. This is how bagging works. It stands for bootstrap aggregating.\n",
    "\n",
    "**Key Points to Remember:**\n",
    "- Bagging reduces variance and helps to avoid overfitting.\n",
    "- It involves creating multiple subsets of the original dataset with replacement, training each subset on the same learning algorithm. (bootstrapping) and then aggregating the predictions from each subset.\n",
    "- A common example of bagging is the: \n",
    "  1. `Random Forest algorithm` which uses bagging as a base algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Boosting**:\n",
    "> Boosting is a method of converting weak learners into strong learners iteratively. Boosting reduces bias and builds strong predictive models.\n",
    "\n",
    "The main principle behind boosting is to fit a sequence of weak learners (models that are only slightly better than random guessing, such as small decision trees) to weighted versions of the data, where more weight is given to examples that were mis-classified by earlier rounds. \n",
    "\n",
    "**A Layman Example for Boosting:**\n",
    "\n",
    "\n",
    "- It adjusts the weight of an observation based on the last classification. \n",
    "- If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa.\n",
    "- Boosting in general decreases the bias error and builds strong predictive models.\n",
    "- **Examples includes:** \n",
    "  1. AdaBoost (Adaptive Boosting)\n",
    "  2. Gradient Boosting (GBM)\n",
    "  3. XGBoost (Extreme Gradient Boosting)\n",
    "  4. LightGBM (Light Gradient Boosting Machine)\n",
    "  5. CatBoost (Categorical Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Stacking**:\n",
    "> Stacking involves training a new model to combine the predictions of several base / existing models.\n",
    "- **It typically involves two levels of models:** \n",
    "  1. Base-level models (at level 0)\n",
    "  2. Meta model (at level 1) \n",
    "     - That makes the final prediction based on the predictions of base models' outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Comparison between Bagging, Boosting, and Stacking:**\n",
    "|  | **Bagging** | **Boosting** | **Stacking** |\n",
    "| --- | --- | --- | --- |\n",
    "| **Purpose** | Reduce variance, avoid overfitting | Reduce bias, improve model accuracy | Improve model performance by combining predictions of multiple different models |\n",
    "| **Base Learner Types** | Homogeneous (often decision trees) | Homogenous (often decision trees) | Heterogenous |\n",
    "| **Base Learner Training** | Parallel (Each model is trained independently on a subset of the data) | Sequential (Each model learning from the mistakes of the previous one) | Meta Model (Each model is trained independently, often on the entire dataset) |\n",
    "| **Aggregation** | Majority voting for classification, averaging for regression | Weighted majority voting for classification, weighted averaging for regression | Weighted Averaging |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why to use Ensemble Methods?**\n",
    "- **Better Predictive Performance**: The primary reason for using ensemble models is to improve predictive performance.\n",
    "- **Accuracy**: It can provide  higher accuracy than single models by combining their strengths and compensating for individual weaknesses.\n",
    "- **Stability**: They are less prone to errors due to the variance in the data, making them more reliable.\n",
    "  - It is more stable and less sensitive to noise.\n",
    "- **Reduces Overfitting**: Techniques like bagging and boosting help to reduce overfitting, making model more generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ensemble Methods Applications:**\n",
    "Ensemble methods are used in wide range of fields such as:\n",
    "- **Finance**: For predicting stock prices, credit risk analysis, etc.\n",
    "- **Healthcare**: For predicting diseases, patient outcomes, etc.\n",
    "- **E-commerce**: For predicting customer behavior, recommendation systems, etc.\n",
    "- **Weather Forecasting**: For predicting weather patterns, etc.\n",
    "- **Marketing**: For predicting customer behavior, customer churn, etc.\n",
    "- **Fraud Detection**: For detecting fraudulent transactions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Challenges with Ensemble Methods:**\n",
    "- **Complexity**: Ensemble methods are complex and require more computational resources.\n",
    "- **Interpretability**: They are less interpretable than single models.\n",
    "- **Training Time**: They require more time to train than single models.\n",
    "- **Parameter Tuning**: They require more careful parameter tuning and selection of appropriate base learners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembal algorithms are a powerful tool in a data scientist's toolkit, offering enhanced accuracy, robustness and generalization. By understanding and utilizing these techniques, one can significantly iprove the performance of their machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
