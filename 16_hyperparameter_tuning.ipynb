{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning**\n",
    "\n",
    "#### **What is Hyperparameter Tuning?**\n",
    "> It is the process of finding the best combination of hyperparameters for a given algorithm. It is a common practice in machine learning to set the hyperparameters of an algorithm before training the model. A hyperparameter is a parameter whose value is used to control the learning process.\n",
    "\n",
    "#### **Types of Hyperparameter Tuning:**\n",
    "1. **Grid Search**: It is a technique that searches for the best combination of hyperparameters from a grid of hyperparameters. It is an exhaustive search technique that tries every possible combination of hyperparameters.\n",
    "2. **Random Search**: It is a technique that searches for the best combination of hyperparameters from a random set of hyperparameters. It is a non-exhaustive search technique that tries a random set of hyperparameters.\n",
    "3. **Bayesian Optimization**: It is a technique that searches for the best combination of hyperparameters using a probabilistic model. It is an iterative search technique that uses a probabilistic model to find the best combination of hyperparameters.\n",
    "4. **Gradient-based Optimization**: It is a technique that searches for the best combination of hyperparameters using gradient descent. It is an iterative search technique that uses gradient descent to find the best combination of hyperparameters.\n",
    "\n",
    "#### **Why is Hyperparameter Tuning Important?**\n",
    "- Hyperparameter tuning is important because it can significantly improve the performance of a machine learning model. By finding the best combination of hyperparameters, we can improve the accuracy, precision, recall, and F1 score of a machine learning model. \n",
    "- Hyperparameter tuning can also help us to avoid overfitting and underfitting by finding the best combination of hyperparameters.\n",
    "\n",
    "#### **Key Concepts of Hyperparameter Tuning:**\n",
    "- **Hyperparameters**: Hyperparameters are the parameters of an algorithm that are set before training the model. They control the learning process of the algorithm.\n",
    "- **Hyperparameter Space**: Hyperparameter space is the space of all possible combinations of hyperparameters. It is the space in which we search for the best combination of hyperparameters.\n",
    "- **Objective Function**: Objective function is the function that we want to optimize. It is the function that we want to maximize or minimize. In the context of hyperparameter tuning, the objective function is the performance metric of the machine learning model.\n",
    "- **Search Technique**: Search technique is the technique that we use to search for the best combination of hyperparameters. There are many search techniques, such as grid search, random search, Bayesian optimization, and gradient-based optimization.\n",
    "- **Validation Set**: Validation set is the set of data that we use to evaluate the performance of the machine learning model. It is the set of data that we use to calculate the performance metric of the machine learning model.\n",
    "- **`Cross-Validation`**: Cross-validation is a technique that we use to evaluate the performance of the machine learning model. It is a technique that we use to calculate the performance metric of the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries:\n",
    "from sklearn.ensemble import RandomForestClassifier # Hyperparameter Tuning using Randome Forest\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # Hyperparamter tuning with GridSearchCV \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Iris Data Set from Sklearn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n",
      "-------------------\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "-------------------\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Import the data using sklearn:\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris)\n",
    "print('-------------------')\n",
    "print(iris.data)\n",
    "print('-------------------')\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Separate the Features (X) and Target (y):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create the Dictionary of Hyperparameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hyperparameter grid:\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],     # Number of trees in random forest\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4, 5, 6, 7, 8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Understanding:**\n",
    "This is a dictionary that defines a grid of hyperparameters for a machine learning model. Each key in the dictionary is a hyperparameter name, and the corresponding value is a list of values that you want to try for that hyperparameter. \n",
    "\n",
    "**Here's what each hyperparameter means:**\n",
    "\n",
    "- `'n_estimators'`: This is the number of trees in the forest. The list `[100, 200, 300, 400]` means you want to try training the model with 100, 200, 300, and 400 trees and see which number gives the best performance.\n",
    "\n",
    "- `'max_features'`: This is the number of features to consider when looking for the best split. The options are `'auto'` (which is equivalent to `'sqrt'`), `'sqrt'` (which means use `sqrt(n_features)` features), and `'log2'` (which means use `log2(n_features)` features).\n",
    "\n",
    "- `'max_depth'`: This is the maximum depth of the tree. The list `[4, 5, 6, 7, 8]` means you want to try training the model with a maximum depth of 4, 5, 6, 7, and 8 and see which depth gives the best performance.\n",
    "\n",
    "- `'criterion'`: This is the function to measure the quality of a split. The options are `'gini'` for the Gini impurity and `'entropy'` for the information gain.\n",
    "\n",
    "This grid will be used in conjunction with a technique like grid search or random search to find the best combination of hyperparameters for your model. The search technique will train a model with each combination of hyperparameters in the grid and select the combination that gives the best performance on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Set up the Grid Search:**\n",
    "- Using the '`GridSearchCV`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the grid search:\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,        # Model to be tuned\n",
    "    param_grid=param_grid,  # Hyperparameter grid\n",
    "    cv=5,                   # 5-fold cross-validations\n",
    "    scoring='accuracy',     # Use accuracy as the evaluation metric\n",
    "    verbose=1,              # Print the result\n",
    "    n_jobs=-1               # Use all available cores\n",
    ")             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a grid search for hyperparameter tuning using the `GridSearchCV` function from the `sklearn.model_selection` module.\n",
    "\n",
    "Here's a breakdown of the parameters:\n",
    "\n",
    "- `estimator=model`: This is the model that you want to tune. The `model` variable should be an instance of a scikit-learn estimator.\n",
    "\n",
    "- `param_grid=param_grid`: This is the grid of hyperparameters that you want to search. The `param_grid` variable should be a dictionary where each key is a hyperparameter name and each value is a list of values to try for that hyperparameter.\n",
    "\n",
    "- `cv=5`: This is the number of folds to use for cross-validation. The data will be split into 5 parts, and the model will be trained and evaluated 5 times so that each part is used as the validation set once.\n",
    "\n",
    "- `scoring='accuracy'`: This is the metric to use for evaluating the models. In this case, it's using accuracy.\n",
    "\n",
    "- `verbose=1`: This controls the verbosity of the output. A value of 1 means it will print the results.\n",
    "\n",
    "- `n_jobs=-1`: This is the number of jobs to run in parallel. A value of `-1` means use all available cores on your machine.\n",
    "\n",
    "> The `GridSearchCV` function will try all combinations of hyperparameters in the grid and perform cross-validation for each combination. It will then select the combination of hyperparameters that gives the best performance according to the scoring metric.\n",
    "\n",
    "In summary, this code is setting up a comprehensive search over the specified parameter grid for a given model, using cross-validation, with the goal of finding the best hyperparameters to maximize the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fit the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "200 fits failed out of a total of 600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "44 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.95333333 0.96\n",
      " 0.96       0.96       0.96       0.96       0.96666667 0.96666667\n",
      "        nan        nan        nan        nan 0.96666667 0.96\n",
      " 0.96       0.96666667 0.96       0.96666667 0.96666667 0.96666667\n",
      "        nan        nan        nan        nan 0.96666667 0.96666667\n",
      " 0.96       0.96666667 0.96666667 0.96       0.96666667 0.96\n",
      "        nan        nan        nan        nan 0.96666667 0.96666667\n",
      " 0.96666667 0.96       0.96       0.96       0.96       0.96\n",
      "        nan        nan        nan        nan 0.96666667 0.96666667\n",
      " 0.96       0.96666667 0.96666667 0.96       0.96       0.96\n",
      "        nan        nan        nan        nan 0.96       0.96666667\n",
      " 0.96       0.96666667 0.95333333 0.96666667 0.96666667 0.96666667\n",
      "        nan        nan        nan        nan 0.96666667 0.96666667\n",
      " 0.96666667 0.96666667 0.96       0.96666667 0.96666667 0.96666667\n",
      "        nan        nan        nan        nan 0.96       0.96\n",
      " 0.96       0.96666667 0.96666667 0.96       0.96       0.96\n",
      "        nan        nan        nan        nan 0.96666667 0.96\n",
      " 0.96666667 0.96666667 0.96       0.96666667 0.96666667 0.96666667\n",
      "        nan        nan        nan        nan 0.95333333 0.96\n",
      " 0.96666667 0.96666667 0.96666667 0.96       0.96666667 0.96      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paramters: {'criterion': 'gini', 'max_depth': 4, 'max_features': 'log2', 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the grid search model:\n",
    "grid.fit(X, y)   # Fit the model with the data \n",
    "\n",
    "# Print the best hyperparameters:\n",
    "print(f\"Best paramters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final step of the grid search process. Here's what it does:\n",
    "\n",
    "- `grid.fit(X, y)`: This line fits the grid search model to your data. The `X` variable is your features, and the `y` variable is your target. The grid search model will try all combinations of hyperparameters in the grid, perform cross-validation for each combination, and select the combination of hyperparameters that gives the best performance according to the scoring metric.\n",
    "\n",
    "- `print(f\"Best paramters: {grid.best_params_}\")`: This line prints the best hyperparameters found by the grid search. The `best_params_` attribute of the grid search model is a dictionary where each key is a hyperparameter name and each value is the best value for that hyperparameter.\n",
    "\n",
    "**Errors:** The output shows that the grid search encountered some errors during the process. Some combinations of hyperparameters caused the model to fail to fit the data. Specifically, the 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. But 'auto' was provided instead.\n",
    "\n",
    "Despite these errors, the grid search was able to find a combination of hyperparameters that worked: `'criterion': 'gini', 'max_depth': 4, 'max_features': 'log2', 'n_estimators': 300`. \n",
    "- This means that the best model found by the grid search is a random forest classifier with `300 trees`, a `maximum depth of 4`, using the '`log2`' rule for the number of features to consider when looking for the best split, and using the Gini impurity as the function to measure the quality of a split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using the '`RandomizedSearchCV`'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "30 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Hp\\miniconda3\\envs\\python_ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.96666667 0.96       0.96666667 0.95333333        nan        nan\n",
      " 0.96666667 0.96666667        nan 0.96666667 0.96666667        nan\n",
      " 0.96666667        nan        nan 0.96       0.96666667 0.96\n",
      " 0.96       0.96      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paramters: {'n_estimators': 300, 'max_features': 'sqrt', 'max_depth': 7, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries:\n",
    "from sklearn.model_selection import RandomizedSearchCV # Hyperparameter Tuning using RandomizedSearchCV\n",
    "\n",
    "# Create the hyperparameter grid:\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],     # Number of trees in random forest\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4, 5, 6, 7, 8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "} \n",
    "\n",
    "# Set up the grid search:\n",
    "grid = RandomizedSearchCV(\n",
    "    estimator=model,        # Model to be tuned\n",
    "    param_distributions=param_grid,  # Hyperparameter grid\n",
    "    cv=5,                   # 5-fold cross-validations\n",
    "    scoring='accuracy',     # Use accuracy as the evaluation metric\n",
    "    verbose=1,              # Print the result\n",
    "    n_jobs=-1,               # Use all available cores\n",
    "    n_iter=20,              # Number of iterations\n",
    ")\n",
    "# Fit the grid search model:\n",
    "grid.fit(X, y)   # Fit the model with the data\n",
    "\n",
    "# Print the best hyperparameters:\n",
    "print(f\"Best paramters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up a randomized search for hyperparameter tuning using the `RandomizedSearchCV` function from the `sklearn.model_selection` module. \n",
    "\n",
    "Here's a breakdown of the parameters:\n",
    "\n",
    "- `estimator=model`: This is the model that you want to tune. The `model` variable should be an instance of a scikit-learn estimator.\n",
    "\n",
    "- `param_distributions=param_grid`: This is the grid of hyperparameters that you want to search. The `param_grid` variable should be a dictionary where each key is a hyperparameter name and each value is a list of values to try for that hyperparameter.\n",
    "\n",
    "- `cv=5`: This is the number of folds to use for cross-validation. The data will be split into 5 parts, and the model will be trained and evaluated 5 times so that each part is used as the validation set once.\n",
    "\n",
    "- `scoring='accuracy'`: This is the metric to use for evaluating the models. In this case, it's using accuracy.\n",
    "\n",
    "- `verbose=1`: This controls the verbosity of the output. A value of 1 means it will print the results.\n",
    "\n",
    "- `n_jobs=-1`: This is the number of jobs to run in parallel. A value of -1 means use all available cores on your machine.\n",
    "\n",
    "- `n_iter=20`: This is the number of parameter settings that are sampled. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values.\n",
    "\n",
    "The `RandomizedSearchCV` function will randomly sample combinations of hyperparameters from the grid, perform cross-validation for each combination, and select the combination of hyperparameters that gives the best performance according to the scoring metric.\n",
    "\n",
    "**Observations from the Output:**\n",
    "> The output shows that the randomized search encountered some errors during the process. Some combinations of hyperparameters caused the model to fail to fit the data. Specifically, the 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. But 'auto' was provided instead.\n",
    "\n",
    "Despite these errors, the randomized search was able to find a combination of hyperparameters that worked: `'n_estimators': 300, 'max_features': 'sqrt', 'max_depth': 7, 'criterion': 'entropy'`. This means that the best model found by the randomized search is a random forest classifier with 300 trees, a maximum depth of 7, using the 'sqrt' rule for the number of features to consider when looking for the best split, and using the entropy as the function to measure the quality of a split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
